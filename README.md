# SupportAI: Dataset Generator and Analyzer

A comprehensive tool for generating and analyzing customer support dialogue datasets using Large Language Models (LLM) and combinatorial augmentation. This project is designed to evaluate support quality, identify agent mistakes, and model complex customer interactions including hidden dissatisfaction.

## ğŸš€ Key Features

- **Synthetic Data Generation**: Creates realistic customer support dialogues across five core intents:
    - `payment_issue`
    - `technical_error`
    - `account_access`
    - `tariff_question`
    - `refund_request`
- **Quality Metrics**: Automatically labels dialogues with:
    - **Intent**: Categorized from predefined sets.
    - **Satisfaction**: Real customer satisfaction level (`satisfied`, `neutral`, `unsatisfied`).
    - **Quality Score**: 1â€“5 scale of agent performance.
    - **Agent Mistakes**: Identification of specific errors like `rude_tone`, `no_resolution`, `ignored_question`, etc.
- **Advanced Augmentation**:
    - **Problematic Cases**: Simulated agent errors and tone issues.
    - **Hidden Dissatisfaction**: Cases where the client formally thanks but remains unsatisfied because the problem persists.
    - **Phrase Variations**: Uses LLM to diversify the vocabulary and tone of interactions.
- **Deterministic Results**: Uses fixed seeds for both Python random and LLM generation to ensure reproducibility.

## ğŸ“ Project Structure

```text
.
â”œâ”€â”€ data/                   # Generated datasets and stats
â”‚   â”œâ”€â”€ dataset.json        # Output: Generated dataset
â”‚   â””â”€â”€ dataset_stats.json  # Output: Generation statistics
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ generator/          # Dataset generation logic
â”‚   â”‚   â”œâ”€â”€ main.py         # Generator entry point
â”‚   â”‚   â””â”€â”€ engine.py       # Core augmentation & LLM logic
â”‚   â”œâ”€â”€ analyzer/           # Dataset analysis logic (placeholder)
â”‚   â”‚   â”œâ”€â”€ main.py         # Analyzer entry point
â”‚   â”‚   â””â”€â”€ engine.py       # Analysis engine
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ constants.py    # Shared constants, mappings, and mistake lists
â”œâ”€â”€ generate.py             # Root script for generation
â”œâ”€â”€ analyze.py              # Root script for analysis
â”œâ”€â”€ pyproject.toml          # Modern Python configuration
â”œâ”€â”€ requirements.txt        # Classic dependencies list
â””â”€â”€ README.md               # Documentation
```

## ğŸ›  Installation

1. **Python**: Version 3.12 or higher.
2. **Ollama**: Ensure Ollama is installed and running locally with the `llama3.1:8b` model.
3. **Setup**:
   ```bash
   # Using uv (recommended)
   uv sync

   # Using pip
   pip install -r requirements.txt
   ```

## ğŸ“‹ Usage

### 1. Generating a Dataset
To generate a new dataset, run the `generate.py` script:
```bash
python generate.py
# or using uv
uv run generate.py
```
This will:
1. Load base data from the Bitext dataset.
2. Enrich it using the LLM.
3. Expand it to the target volume (default: 100 dialogues for testing).
4. Save the results to `data/dataset.json`.

### 2. Analyzing a Dataset
To analyze existing dialogues:
```bash
python analyze.py
```
*(Note: Analysis logic is currently a placeholder and will be implemented in the next phase.)*

## âš™ï¸ Configuration

- **Model**: Change the `ollama_model` in `src/generator/main.py`.
- **Intents and Mistakes**: Edit `src/config/constants.py` to add new categories or mistake types.
- **Target Size**: Adjust `target_count` in `src/generator/main.py` for larger datasets.

## ğŸ“Š Evaluation Criteria

This project is built to satisfy the following evaluation criteria:
- **Realism**: Diverse scenarios and natural-sounding variations.
- **Complexity**: Detection of hidden dissatisfaction and subtle agent errors.
- **Structure**: Clean modular architecture and standardized JSON output.
