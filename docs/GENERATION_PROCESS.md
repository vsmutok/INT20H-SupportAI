# Процес генерації датасету

Опис того, як `generate.py` будує фінальний датасет діалогів підтримки.

## Загальна ідея

Беремо реальні діалоги з HuggingFace (Bitext), збагачуємо їх метаданими через LLM,
генеруємо варіації фраз і потім розширюємо до потрібного розміру,
комбінуючи різні типи сценаріїв (успішні, проблемні, конфліктні тощо).

## Кроки

### 1. Завантаження базового датасету

`DatasetAugmenter.load_base_dataset()` завантажує
`bitext/Bitext-customer-support-llm-chatbot-training-dataset` з HuggingFace.

Кожен семпл перетворюється в діалог формату `client → agent`.
Категорії Bitext маппляться на наші інтенти через `INTENT_MAP` в `constants.py`:

- `payment_issue`, `technical_error`, `account_access`
- `tariff_question`, `refund_request`, `other`

Шаблонні змінні (`{{Order Number}}`, `{{Person Name}}` тощо) замінюються
на реалістичні значення з `TEMPLATE_REPLACEMENTS`.

### 2. Аналіз вибірки через LLM

Перші N діалогів (за замовчуванням 1000) проганяються через Llama 3.1 8b батчами по 10.
Для кожного діалогу LLM визначає:

- **satisfaction** — `satisfied` / `neutral` / `unsatisfied`
- **quality_score** — 1–5
- **agent_mistakes** — список з `AGENT_MISTAKES` або порожній

Для решти діалогів метадані генеруються статистично (рандом зі зваженим розподілом).

### 3. Генерація варіацій фраз

`augment_with_variations()` бере до 1000 унікальних фраз клієнтів і 1000 фраз агентів,
для кожної генерує 3 синонімічні варіації через LLM (з різною тональністю).
Результати кешуються в `analysis_cache` і використовуються при розширенні.

### 4. Розширення до цільового розміру

`expand_dataset()` створює нові діалоги п'яти типів:

| Тип | Частка | Опис |
|-----|--------|------|
| Successful | 45% | Підставляються згенеровані варіації фраз, довжина 2–7 повідомлень |
| Problematic | 25% | Агент робить одну з помилок (`rude_tone`, `no_resolution` тощо), генерується через LLM |
| Conflict | 15% | Серйозні конфлікти: кілька помилок агента, `quality_score` 1–2 |
| Hidden dissatisfaction | 10% | Клієнт відповідає ввічливо ("Okay, thanks"), але проблема не вирішена |
| Multi-turn | 5% | Довгі діалоги (6–7 повідомлень) з LLM-генерованими follow-up репліками |

Діалоги розширюються до потрібної кількості повідомлень через `_extend_dialog_turns()`.
Якщо LLM не згенерував достатньо реплік — підставляються шаблонні фрази як fallback.

Кожен діалог отримує детерміністичний `id` (md5 від типу + seed + індекс).

### 5. Збереження

- `data/dataset.json` — фінальний датасет
- `data/dataset_stats.json` — статистика: розподіл за інтентами, satisfaction, помилками, довжиною діалогів

## Файли

| Файл | Опис |
|------|-----------|
| `generate.py` | Точка входу: запуск процесу генерації. |
| `src/generator/main.py` | Оркестрація: завантаження → аналіз → варіації → розширення → збереження. |
| `src/generator/engine.py` | `DatasetAugmenter`: робота з базою, LLM-запити, створення проблемних сценаріїв. |
| `src/generator/prompts.py` | Шаблони промптів для генерації, варіацій та доповнення діалогів. |
| `src/config/constants.py` | Константи: `SEED`, `INTENT_MAP`, `AGENT_MISTAKES`, `TEMPLATE_REPLACEMENTS`. |
| `src/config/logger.py` | Налаштування логування для відстеження кроків генерації. |

## Конфігурація

Всі основні налаштування знаходяться в `src/config/constants.py`:
- `SEED`: зерно для детермінізму (за замовчуванням 42)
- `INTENT_MAP`: мапінг категорій Bitext на внутрішні інтенти
- `AGENT_MISTAKES`: список можливих помилок агента

Також можна налаштувати підключення до Ollama через змінну оточення `OLLAMA_HOST` (корисно для Docker).
